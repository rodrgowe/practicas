{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se recomienda instalar las siguientes librerías\n",
    "# pip install pandas\n",
    "# pip install tensorflow\n",
    "# pip install matplotlib\n",
    "# pip install optuna\n",
    "# pip install scikit-learn\n",
    "# pip install keras\n",
    "# pip install tensorflow\n",
    "# pip install scipy\n",
    "# pip install python-pptx\n",
    "#IMPORTACION DE LAS LIBRERIAS A UTILIZAR\n",
    "\n",
    "import pandas as pd #librería para analizar datos\n",
    "import numpy as np #librería para algebra linear\n",
    "import tensorflow as tf #librería para modelos de deep learning\n",
    "import matplotlib.pyplot as plt # libreraía para realizar los graficos\n",
    "import optuna # libreria para optimizar modelo\n",
    "from sklearn.model_selection import train_test_split #parte de la librería que divide los datos\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # libreria para calcular las metricas en la curva ROC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # librerias para calcular la precision, el recall y el f1 score\n",
    "from sklearn.preprocessing import MinMaxScaler #libreria para estandarizacion de datos\n",
    "from sklearn.preprocessing import StandardScaler #libreria para estandarizacion de datos \n",
    "from keras.models import Sequential #importar el inicio de un modelo secuencial\n",
    "from keras.layers import LSTM, Dense # importa capas densas y de LSTM\n",
    "from sklearn.metrics import confusion_matrix #parte de la librería para hacer la matriz de confusion\n",
    "from tensorflow import keras #importa keras que se encarga de la transformacion de los datos\n",
    "from spicy import stats\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from pptx import Presentation\n",
    "from io import BytesIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping \n",
    "numero = 10 #numero de bootstrapping\n",
    "data5 = pd.DataFrame()\n",
    "ruut = r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta (2)' #carpeta donde se encuentran los excels\n",
    "g = 0\n",
    "\n",
    "# Ruta donde se guardarán las diapositivas\n",
    "ruta_presentacion = r\"C:\\Users\\colazabal\\Desktop\\Nueva carpeta (2)\" # donde guardaremos la ppt\n",
    "# Crear una presentación vacía\n",
    "ppt = Presentation()\n",
    "\n",
    "#LOOP PARA CADA EXCEL QUE ESTÉ EN LA RUTA\n",
    "for x in os.listdir(ruut):\n",
    "\n",
    "    if x.endswith('xlsx'):\n",
    "        root = os.path.join(ruut, x)\n",
    "        data = pd.read_excel(root)\n",
    "\n",
    "        for y in data.columns[1:]:\n",
    "            if len(data[y].dropna()) < 70:\n",
    "                g = g + 1\n",
    "\n",
    "                data1 = data[y].dropna().diff()\n",
    "                data1 = data1[1:]\n",
    "                replica = []\n",
    "                print(len(data1))\n",
    "                for dato in range(numero):\n",
    "                    bootstrap_replica = np.random.choice(data1, len(data1),replace=True)\n",
    "                    replica.append(bootstrap_replica)\n",
    "\n",
    "                df = pd.DataFrame({f'{y}': replica})\n",
    "                df = df.explode(f'{y}').reset_index(drop=True)\n",
    "                data1 = pd.DataFrame(data1)\n",
    "                data2 = pd.concat([data1,df], axis=0).reset_index(drop=True)\n",
    "                primer_valor = data[y].dropna().reset_index(drop=True)[0]\n",
    "                data2['acum'] = data2[f'{y}'].cumsum()\n",
    "                data2['serie acum'] = primer_valor + data2['acum']\n",
    "                data5[y] = data2['serie acum']\n",
    "                \n",
    "                try: \n",
    "                    empresa = y\n",
    "\n",
    "                    valor1 = len(data5[empresa].dropna())\n",
    "                    valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "                    TIME_STEPS = 30 #valor que se puede variar en el tiempo - CONSULTAR A MARVIN SI SE PUEDE REDUCIR EL NUMERO PARA SERIES PEQUEÑAS.\n",
    "                    epochs = 100 \n",
    "                    batch_size = 32\n",
    "\n",
    "                    #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "                    #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "                    data5['valor_normalizado'] = data5[empresa].dropna().reset_index(drop = True)\n",
    "                    scaler = StandardScaler()\n",
    "                    scaler = scaler.fit(data5[['valor_normalizado']])\n",
    "                    data5['valor_normalizado'] = scaler.transform(data5[['valor_normalizado']])\n",
    "\n",
    "                    def create_dataset(X, y, time_steps=1):\n",
    "                        Xs, ys = [], []\n",
    "                        for i in range(len(X) - time_steps):\n",
    "                            v = X.iloc[i:(i + time_steps)].values\n",
    "                            Xs.append(v)        \n",
    "                            ys.append(y.iloc[i + time_steps])\n",
    "                        return np.array(Xs), np.array(ys)\n",
    "                    \n",
    "\n",
    "\n",
    "                    X_data, y_data = create_dataset(data5[['valor_normalizado']], data5.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "                    #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "                    model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                        tf.keras.layers.Dense(64, activation='relu'),\n",
    "                        tf.keras.layers.Dense(32, activation='relu'),\n",
    "                        tf.keras.layers.Dense(16, activation='relu'),\n",
    "                        tf.keras.layers.Dense(32, activation='relu'),\n",
    "                        tf.keras.layers.Dense(64, activation='relu'),\n",
    "                        tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "                    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "                    history = model.fit(X_data, X_data, epochs=epochs, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0) #ENTRENAMIENTO\n",
    "                    mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "\n",
    "                    #predicciones\n",
    "                    predictions = model.predict(X_data)\n",
    "                    predicciones1 = pd.DataFrame(predictions.flat[0:TIME_STEPS])\n",
    "\n",
    "                    predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "                    predicciones = pd.concat((predicciones1, predicciones2),axis=0)\n",
    "\n",
    "                    predicciones.reset_index(drop=True, inplace=True)\n",
    "                    \n",
    "                    #calculo del error de reconstrucción\n",
    "                    df_test = pd.DataFrame()\n",
    "                    df_test['x_test'] = data5['valor_normalizado']\n",
    "                    df_test['predic'] = predicciones\n",
    "                    df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "                    df_test['error2'] = df_test['error']**2\n",
    "\n",
    "                    #cálculo del threshold\n",
    "                    mu = np.mean(df_test['error2'])  # Media\n",
    "                    sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "                    alpha = 0.05  # Nivel de significancia del 5%\n",
    "                    z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "                    threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "                    anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "\n",
    "                    def objective(trial):\n",
    "                        # Definir los espacios de búsqueda para los hiperparámetros\n",
    "                        num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "                        num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "                        num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "                        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "                        # Crear un modelo Autoencoder\n",
    "                        model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                            tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                            tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                            tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "                            tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                            tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                            tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "                        # Compilar el modelo con los hiperparámetros seleccionados\n",
    "                        model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "                        # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "                        history = model.fit(X_data, X_data, epochs=100, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0)\n",
    "\n",
    "                        # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "                        # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "                        loss = model.evaluate(X_data, X_data)  # Corregido\n",
    "\n",
    "                        return loss\n",
    "\n",
    "                    # Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "                    study = optuna.create_study(direction='minimize')\n",
    "                    study.optimize(objective, n_trials=10)\n",
    "\n",
    "                    # Obtener los mejores hiperparámetros encontrados\n",
    "                    best_params = study.best_params\n",
    "\n",
    "                    # Imprimir los mejores hiperparámetros\n",
    "                    print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "                    #UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "                    #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "                    model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                        tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                        tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                        tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "                        tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                        tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                        tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "                    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "                    history = model.fit(X_data, X_data, epochs=epochs, batch_size=best_params['batch_size'], validation_data=(X_data, X_data), verbose=0)\n",
    "                    mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "\n",
    "\n",
    "                    predictions = model.predict(X_data)\n",
    "                    predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "                    predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "                    predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "                    predicciones.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "\n",
    "                    df_test = pd.DataFrame()\n",
    "                    df_test['x_test'] = data5['valor_normalizado']\n",
    "                    df_test['predic'] = predicciones\n",
    "                    df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "                    df_test['error2'] = df_test['error']**2\n",
    "\n",
    "                    df_test = df_test.iloc[:len(data1)]\n",
    "                    anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "                    \n",
    "                    # Crear una nueva figura para el segundo gráfico\n",
    "\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.plot(data5['valor_normalizado'][:len(data1)], label='Datos Originales', color='blue')\n",
    "                    plt.title(f'{empresa}')\n",
    "                    plt.xlabel('Índice de Datos')\n",
    "                    plt.ylabel('Valor Normalizado')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    buf2 = BytesIO()\n",
    "                    plt.savefig(buf2, format='png')\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "                    left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "                    slide = ppt.slides.add_slide(ppt.slide_layouts[5])\n",
    "                    pic2 = slide.shapes.add_picture(buf2, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "                    \n",
    "                    # Crear una figura y un solo eje y\n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                    # Gráfico de Datos Originales y Predicciones\n",
    "                    ax.plot(data5['valor_normalizado'].iloc[:len(data1)], label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "                    ax.plot(predicciones.iloc[:len(data1)], label='Predicciones', color='black')\n",
    "                    ax.scatter(anomalias.index[:len(data1)], anomalias['x_test'].iloc[:len(data1)], color='red', label='Anomalías', marker='o', s=20)\n",
    "                    ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "                    ax.set_xlabel('Datos de Testeo')\n",
    "                    ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "                    ax.legend(loc='upper left')\n",
    "\n",
    "                    # Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "                    ax2 = ax.twinx()\n",
    "                    ax2.plot(df_test.index[:len(data1)], df_test.error2.iloc[:len(data1)], label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "                    ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "                    ax2.legend(loc='upper right')\n",
    "                    y2_min = 0\n",
    "                    y2_max = max(df_test.error2)*4\n",
    "\n",
    "                    # Establecer los límites en el eje y del segundo conjunto de datos\n",
    "                    ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "\n",
    "                    # Ajustar el espaciado entre los gráficos\n",
    "                    plt.tight_layout()\n",
    "                    buf = BytesIO()\n",
    "                    plt.savefig(buf, format='png')\n",
    "                    # Mostrar el gráfico combinado\n",
    "                    plt.show()\n",
    "                    # Crear una nueva diapositiva\n",
    "                    slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # 5 es el diseño de título y contenido\n",
    "\n",
    "                    # Insertar la primera imagen del gráfico\n",
    "                    left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "                    pic1 = slide.shapes.add_picture(buf, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "                except:\n",
    "                    print(f'error con el archivo {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se recomienda instalar las siguientes librerías\n",
    "# pip install pandas\n",
    "# pip install tensorflow\n",
    "# pip install matplotlib\n",
    "# pip install optuna\n",
    "# pip install scikit-learn\n",
    "# pip install keras\n",
    "# pip install tensorflow\n",
    "# pip install scipy\n",
    "# pip install python-pptx\n",
    "\n",
    "#IMPORTACION DE LAS LIBRERIAS A UTILIZAR\n",
    "\n",
    "import pandas as pd #librería para analizar datos\n",
    "import numpy as np #librería para algebra linear\n",
    "import tensorflow as tf #librería para modelos de deep learning\n",
    "import matplotlib.pyplot as plt # libreraía para realizar los graficos\n",
    "import optuna # libreria para optimizar modelo\n",
    "from sklearn.model_selection import train_test_split #parte de la librería que divide los datos\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # libreria para calcular las metricas en la curva ROC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # librerias para calcular la precision, el recall y el f1 score\n",
    "from sklearn.preprocessing import MinMaxScaler #libreria para estandarizacion de datos\n",
    "from sklearn.preprocessing import StandardScaler #libreria para estandarizacion de datos \n",
    "from keras.models import Sequential #importar el inicio de un modelo secuencial\n",
    "from keras.layers import LSTM, Dense # importa capas densas y de LSTM\n",
    "from sklearn.metrics import confusion_matrix #parte de la librería para hacer la matriz de confusion\n",
    "from tensorflow import keras #importa keras que se encarga de la transformacion de los datos\n",
    "from spicy import stats\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from pptx import Presentation\n",
    "from io import BytesIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# series normales \n",
    "numero = 10 #cantidad de replicas de los datos\n",
    "data5 = pd.DataFrame()\n",
    "ruut = r'\\\\srvfiledfs\\documentos\\DPRC\\2023\\FINANZAS\\COLAZABAL\\14. Recurrent Neural Network\\Nueva carpeta' #carpeta donde se encuentran los excels\n",
    "g = 0\n",
    "# Ruta donde se guardarán las diapositivas\n",
    "ruta_presentacion = r\"C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\presentacion\" # donde guardaremos la ppt\n",
    "# Crear una presentación vacía\n",
    "ppt = Presentation()\n",
    "\n",
    "#en estos casos no se realiza bootstrap solo se \n",
    "\n",
    "for x in os.listdir(ruut):\n",
    "    if x.endswith('xlsx'):\n",
    "        root = os.path.join(ruut, x)\n",
    "        data = pd.read_excel(root)\n",
    "        for y in data.columns[1:]:             \n",
    "            try: \n",
    "                empresa = y\n",
    "\n",
    "                valor1 = len(data5[empresa].dropna())\n",
    "                valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "                TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "                epochs = 100 \n",
    "                batch_size = 32 \n",
    "\n",
    "                #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "                data5['valor_normalizado'] = data5[empresa].dropna().reset_index(drop = True)\n",
    "                scaler = StandardScaler()\n",
    "                scaler = scaler.fit(data5[['valor_normalizado']])\n",
    "                data5['valor_normalizado'] = scaler.transform(data5[['valor_normalizado']])\n",
    "\n",
    "                def create_dataset(X, y, time_steps=1):\n",
    "                    Xs, ys = [], []\n",
    "                    for i in range(len(X) - time_steps):\n",
    "                        v = X.iloc[i:(i + time_steps)].values\n",
    "                        Xs.append(v)        \n",
    "                        ys.append(y.iloc[i + time_steps])\n",
    "                    return np.array(Xs), np.array(ys)\n",
    "\n",
    "                X_data, y_data = create_dataset(data5[['valor_normalizado']], data5.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "                #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                    tf.keras.layers.Dense(32, activation='relu'),\n",
    "                    tf.keras.layers.Dense(16, activation='relu'),\n",
    "                    tf.keras.layers.Dense(32, activation='relu'),\n",
    "                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                    tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "                history = model.fit(X_data, X_data, epochs=epochs, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0) #ENTRENAMIENTO\n",
    "                mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "                loss = history.history['loss']\n",
    "                val_loss = history.history['val_loss']\n",
    "\n",
    "                #predicciones\n",
    "                predictions = model.predict(X_data)\n",
    "                predicciones1 = pd.DataFrame(predictions.flat[0:TIME_STEPS])\n",
    "\n",
    "                predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "                predicciones = pd.concat((predicciones1, predicciones2),axis=0)\n",
    "\n",
    "                predicciones.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                #calculo del error de reconstrucción\n",
    "                df_test = pd.DataFrame()\n",
    "                df_test['x_test'] = data5['valor_normalizado']\n",
    "                df_test['predic'] = predicciones\n",
    "                df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "                df_test['error2'] = df_test['error']**2\n",
    "\n",
    "                #cálculo del threshold\n",
    "                mu = np.mean(df_test['error2'])  # Media\n",
    "                sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "                alpha = 0.05  # Nivel de significancia del 5%\n",
    "                z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "                threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "                anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "\n",
    "                def objective(trial):\n",
    "                    # Definir los espacios de búsqueda para los hiperparámetros\n",
    "                    num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "                    num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "                    num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "                    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "                    # Crear un modelo Autoencoder\n",
    "                    model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                        tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "                    # Compilar el modelo con los hiperparámetros seleccionados\n",
    "                    model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "                    # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "                    history = model.fit(X_data, X_data, epochs=100, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0)\n",
    "\n",
    "                    # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "                    # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "                    loss = model.evaluate(X_data, X_data)  # Corregido\n",
    "\n",
    "                    return loss\n",
    "\n",
    "                # Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "                study = optuna.create_study(direction='minimize')\n",
    "                study.optimize(objective, n_trials=10)\n",
    "\n",
    "                # Obtener los mejores hiperparámetros encontrados\n",
    "                best_params = study.best_params\n",
    "\n",
    "                # Imprimir los mejores hiperparámetros\n",
    "                print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "                #UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "                #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                    tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "                    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                    tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "                history = model.fit(X_data, X_data, epochs=epochs, batch_size=best_params['batch_size'], validation_data=(X_data, X_data), verbose=0)\n",
    "                mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "\n",
    "\n",
    "                predictions = model.predict(X_data)\n",
    "                predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "                predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "                predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "                predicciones.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                loss = history.history['loss']\n",
    "                val_loss = history.history['val_loss']\n",
    "\n",
    "                df_test = pd.DataFrame()\n",
    "                df_test['x_test'] = data5['valor_normalizado']\n",
    "                df_test['predic'] = predicciones\n",
    "                df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "                df_test['error2'] = df_test['error']**2\n",
    "\n",
    "                df_test = df_test.iloc[:len(data1)]\n",
    "                anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "                \n",
    "                # Crear una nueva figura para el segundo gráfico\n",
    "\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.plot(data5['valor_normalizado'][:len(data1)], label='Datos Originales', color='blue')\n",
    "                plt.title(f'{empresa}')\n",
    "                plt.xlabel('Índice de Datos')\n",
    "                plt.ylabel('Valor Normalizado')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                buf2 = BytesIO()\n",
    "                plt.savefig(buf2, format='png')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "                slide = ppt.slides.add_slide(ppt.slide_layouts[5])\n",
    "                pic2 = slide.shapes.add_picture(buf2, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "                \n",
    "                # Crear una figura y un solo eje y\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                # Gráfico de Datos Originales y Predicciones\n",
    "                ax.plot(data5['valor_normalizado'].iloc[:len(data1)], label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "                ax.plot(predicciones.iloc[:len(data1)], label='Predicciones', color='black')\n",
    "                ax.scatter(anomalias.index[:len(data1)], anomalias['x_test'].iloc[:len(data1)], color='red', label='Anomalías', marker='o', s=20)\n",
    "                ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "                ax.set_xlabel('Datos de Testeo')\n",
    "                ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "                ax.legend(loc='upper left')\n",
    "\n",
    "                # Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.plot(df_test.index[:len(data1)], df_test.error2.iloc[:len(data1)], label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "                ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "                ax2.legend(loc='upper right')\n",
    "                y2_min = 0\n",
    "                y2_max = max(df_test.error2)*4\n",
    "\n",
    "                # Establecer los límites en el eje y del segundo conjunto de datos\n",
    "                ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "\n",
    "                # Ajustar el espaciado entre los gráficos\n",
    "                plt.tight_layout()\n",
    "                buf = BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                # Mostrar el gráfico combinado\n",
    "                plt.show()\n",
    "                # Crear una nueva diapositiva\n",
    "                slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # 5 es el diseño de título y contenido\n",
    "\n",
    "                # Insertar la primera imagen del gráfico\n",
    "                left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "                pic1 = slide.shapes.add_picture(buf, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "            except:\n",
    "                print(f'error con el archivo {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt.save(r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\pptnuevo4.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta2 = r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\presentacion'\n",
    "# Ruta donde se guardarán las diapositivas\n",
    "ruta_presentacion = r\"C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\presentacion\"\n",
    "# Crear una presentación vacía\n",
    "ppt = Presentation()\n",
    "\n",
    "for x in data5.columns:\n",
    "        try: \n",
    "            empresa = x\n",
    "\n",
    "            valor1 = len(data5[empresa].dropna())\n",
    "            valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "            TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "            epochs = 100 \n",
    "            batch_size = 32\n",
    "\n",
    "            #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "            #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "            data5['valor_normalizado'] = data5[empresa].dropna().reset_index(drop = True)\n",
    "            scaler = StandardScaler()\n",
    "            scaler = scaler.fit(data5[['valor_normalizado']])\n",
    "            data5['valor_normalizado'] = scaler.transform(data5[['valor_normalizado']])\n",
    "\n",
    "            def create_dataset(X, y, time_steps=1):\n",
    "                Xs, ys = [], []\n",
    "                for i in range(len(X) - time_steps):\n",
    "                    v = X.iloc[i:(i + time_steps)].values\n",
    "                    Xs.append(v)        \n",
    "                    ys.append(y.iloc[i + time_steps])\n",
    "                return np.array(Xs), np.array(ys)\n",
    "            \n",
    "\n",
    "\n",
    "            X_data, y_data = create_dataset(data5[['valor_normalizado']], data5.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "            #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(16, activation='relu'),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "            history = model.fit(X_data, X_data, epochs=epochs, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0) #ENTRENAMIENTO\n",
    "            mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "\n",
    "            #predicciones\n",
    "            predictions = model.predict(X_data)\n",
    "            predicciones1 = pd.DataFrame(predictions.flat[0:TIME_STEPS])\n",
    "\n",
    "            predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "            predicciones = pd.concat((predicciones1, predicciones2),axis=0)\n",
    "\n",
    "            predicciones.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            #calculo del error de reconstrucción\n",
    "            df_test = pd.DataFrame()\n",
    "            df_test['x_test'] = data5['valor_normalizado']\n",
    "            df_test['predic'] = predicciones\n",
    "            df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "            df_test['error2'] = df_test['error']**2\n",
    "\n",
    "            #cálculo del threshold\n",
    "            mu = np.mean(df_test['error2'])  # Media\n",
    "            sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "            alpha = 0.05  # Nivel de significancia del 5%\n",
    "            z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "            threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "            anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "\n",
    "            def objective(trial):\n",
    "                # Definir los espacios de búsqueda para los hiperparámetros\n",
    "                num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "                num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "                num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "                batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "                # Crear un modelo Autoencoder\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                    tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                    tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                    tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "                    tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "                    tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "                    tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "                # Compilar el modelo con los hiperparámetros seleccionados\n",
    "                model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "                # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "                history = model.fit(X_data, X_data, epochs=100, batch_size=batch_size, validation_data=(X_data, X_data), verbose=0)\n",
    "\n",
    "                # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "                # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "                loss = model.evaluate(X_data, X_data)  # Corregido\n",
    "\n",
    "                return loss\n",
    "\n",
    "            # Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=2)\n",
    "\n",
    "            # Obtener los mejores hiperparámetros encontrados\n",
    "            best_params = study.best_params\n",
    "\n",
    "            # Imprimir los mejores hiperparámetros\n",
    "            print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "            #UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "            #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_data.shape[1])),\n",
    "                tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "                tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "                tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "                tf.keras.layers.Dense(X_data.shape[1], activation='linear')])\n",
    "\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "            history = model.fit(X_data, X_data, epochs=epochs, batch_size=best_params['batch_size'], validation_data=(X_data, X_data), verbose=0)\n",
    "            mse = model.evaluate(X_data, X_data, verbose=0)\n",
    "\n",
    "\n",
    "            predictions = model.predict(X_data)\n",
    "            predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "            predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "            predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "            predicciones.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "\n",
    "            df_test = pd.DataFrame()\n",
    "            df_test['x_test'] = data5['valor_normalizado']\n",
    "            df_test['predic'] = predicciones\n",
    "            df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "            df_test['error2'] = df_test['error']**2\n",
    "\n",
    "            anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "            \n",
    "            # Crear una nueva figura para el segundo gráfico\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(data5['valor_normalizado'][:len(data5[empresa])], label='Datos Originales', color='blue')\n",
    "            plt.title(f'{empresa}')\n",
    "            plt.xlabel('Índice de Datos')\n",
    "            plt.ylabel('Valor Normalizado')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            buf2 = BytesIO()\n",
    "            plt.savefig(buf2, format='png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "            slide = ppt.slides.add_slide(ppt.slide_layouts[5])\n",
    "            pic2 = slide.shapes.add_picture(buf2, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "            \n",
    "            # Crear una figura y un solo eje y\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            # Gráfico de Datos Originales y Predicciones\n",
    "            ax.plot(data5['valor_normalizado'][:len(data5[empresa])], label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "            ax.plot(predicciones[:len(data5[empresa])], label='Predicciones', color='black')\n",
    "            ax.scatter(anomalias.index[:len(data5[empresa])], anomalias['x_test'][:len(data5[empresa])], color='red', label='Anomalías', marker='o', s=20)\n",
    "            ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "            ax.set_xlabel('Datos de Testeo')\n",
    "            ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "            # Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(df_test.index[:len(data5[empresa])], df_test.error2[:len(data5[empresa])], label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "            ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "            ax2.legend(loc='upper right')\n",
    "            y2_min = 0\n",
    "            y2_max = max(df_test.error2)*4\n",
    "\n",
    "            # Establecer los límites en el eje y del segundo conjunto de datos\n",
    "            ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "\n",
    "            # Ajustar el espaciado entre los gráficos\n",
    "            plt.tight_layout()\n",
    "            buf = BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            # Mostrar el gráfico combinado\n",
    "            plt.show()\n",
    "            # Crear una nueva diapositiva\n",
    "            slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # 5 es el diseño de título y contenido\n",
    "\n",
    "            # Insertar la primera imagen del gráfico\n",
    "            left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "            pic1 = slide.shapes.add_picture(buf, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "        except:\n",
    "            print(f'error con el archivo {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt.save(r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\pptnuevo3.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta2 = r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\presentacion'\n",
    "# Ruta donde se guardarán las diapositivas\n",
    "ruta_presentacion = r\"C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\presentacion\"\n",
    "\n",
    "# Crear una presentación vacía\n",
    "ppt = Presentation()\n",
    "\n",
    "for x in data5.columns:\n",
    "        \n",
    "    empresa = x\n",
    "\n",
    "\n",
    "    test_size = 0.3 # longitud del testeo\n",
    "    valor = 1 - test_size\n",
    "    valor1 = valor*400\n",
    "    valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "    TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "    epochs = 100 \n",
    "    batch_size = 32\n",
    "\n",
    "    #PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "    data['valor_normalizado'] = data5[empresa]\n",
    "    train, test = train_test_split(data['valor_normalizado'], test_size=test_size, shuffle = False)\n",
    "    train = pd.DataFrame(train)\n",
    "    test= pd.DataFrame(test)\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(train[['valor_normalizado']])\n",
    "    train['valor_normalizado'] = scaler.transform(train[['valor_normalizado']])\n",
    "    test['valor_normalizado'] = scaler.transform(test[['valor_normalizado']])\n",
    "\n",
    "    def create_dataset(X, y, time_steps=1):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            v = X.iloc[i:(i + time_steps)].values\n",
    "            Xs.append(v)        \n",
    "            ys.append(y.iloc[i + time_steps])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    X_train, y_train = create_dataset(train[['valor_normalizado']], train.valor_normalizado, TIME_STEPS)\n",
    "    X_test, y_test = create_dataset(test[['valor_normalizado']], test.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "    #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0) #ENTRENAMIENTO\n",
    "    mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # desprocesamiento de los datos de testeo\n",
    "    a = pd.DataFrame(X_test[0].flat)\n",
    "    b = pd.DataFrame(y_test)\n",
    "    datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "    #predicciones\n",
    "    predictions = model.predict(X_test)\n",
    "    predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "    predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "    predicciones = pd.concat((predicciones1, predicciones2))\n",
    "    predicciones.reset_index(drop=True, inplace=True)\n",
    "    datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #calculo del error de reconstrucción\n",
    "    df_test = pd.DataFrame()\n",
    "    df_test['x_test'] = datos_testeo\n",
    "    df_test['predic'] = predicciones\n",
    "    df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "    df_test['error2'] = df_test['error']**2\n",
    "\n",
    "    #cálculo del threshold\n",
    "    mu = np.mean(df_test['error2'])  # Media\n",
    "    sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "    alpha = 0.05  # Nivel de significancia del 5%\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "    threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "    anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "\n",
    "    def objective(trial):\n",
    "        # Definir los espacios de búsqueda para los hiperparámetros\n",
    "        num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "        num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "        num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "        # Crear un modelo Autoencoder\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "            tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "            tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "        # Compilar el modelo con los hiperparámetros seleccionados\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "        # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "        history = model.fit(X_train, X_train, epochs=100, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0)\n",
    "\n",
    "        # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "        # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "        loss = model.evaluate(X_train, X_train)  # Corregido\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    # Obtener los mejores hiperparámetros encontrados\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Imprimir los mejores hiperparámetros\n",
    "    print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "    #UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "    #CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "        tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "        tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "        tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "        tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "        tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "        tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history = model.fit(X_train, X_train, epochs=50, batch_size=best_params['batch_size'], validation_data=(X_train, X_train), verbose=0)\n",
    "    mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "\n",
    "    a = pd.DataFrame(X_test[0].flat)\n",
    "    b = pd.DataFrame(y_test)\n",
    "    datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "    predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "    predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "    predicciones.reset_index(drop=True, inplace=True)\n",
    "    datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    df_test = pd.DataFrame()\n",
    "    df_test['x_test'] = datos_testeo\n",
    "    df_test['predic'] = predicciones\n",
    "    df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "    df_test['error2'] = df_test['error']**2\n",
    "\n",
    "    anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "    # Crear una nueva figura para el segundo gráfico\n",
    "                \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data['valor_normalizado'], label='Datos Originales', color='blue')\n",
    "    plt.title(f'{os.path.splitext(x)[0]} - {y}')\n",
    "    plt.xlabel('Índice de Datos')\n",
    "    plt.ylabel('Valor Normalizado')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Guardar la segunda figura en un BytesIO\n",
    "    buf2 = BytesIO()\n",
    "    plt.savefig(buf2, format='png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Insertar la segunda imagen del gráfico\n",
    "    slide = ppt.slides.add_slide(ppt.slide_layouts[5])\n",
    "    left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "    pic2 = slide.shapes.add_picture(buf2, left1, top1, width=ppt.slide_width, height=ppt.slide_height)\n",
    "\n",
    "    # Crear una figura y un solo eje y\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    # Gráfico de Datos Originales y Predicciones\n",
    "    ax.plot(data['valor_normalizado'], label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "    ax.plot(predicciones, label='Predicciones', color='black')\n",
    "    ax.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=20)\n",
    "    ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "    ax.set_xlabel('Datos de Testeo')\n",
    "    ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "    ax.legend(loc='upper left')\n",
    "    # Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df_test.index, df_test.error2, label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "    ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "    ax2.legend(loc='upper right')\n",
    "    y2_min = 0\n",
    "    y2_max = max(df_test.error2)*4\n",
    "    # Establecer los límites en el eje y del segundo conjunto de datos\n",
    "    ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "    # Ajustar el espaciado entre los gráficos\n",
    "    plt.tight_layout()\n",
    "    # Mostrar el gráfico combinado\n",
    "\n",
    "    # Guardar la figura en un BytesIO para luego insertarla en la diapositiva\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Crear una nueva diapositiva\n",
    "    slide = ppt.slides.add_slide(ppt.slide_layouts[5])  # 5 es el diseño de título y contenido\n",
    "\n",
    "    # Insertar la primera imagen del gráfico\n",
    "    left1, top1 = 0, 0  # Ajusta la posición de la primera imagen\n",
    "    pic1 = slide.shapes.add_picture(buf, left1, top1, width=ppt.slide_width, height=ppt.slide_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la presentación\n",
    "ppt.save(r'C:\\Users\\colazabal\\Desktop\\Nueva carpeta\\ppt.pptx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in os.listdir(ruta):\n",
    "    if x.endswith('xlsx'):\n",
    "        print(x)\n",
    "        wd = os.path.join(ruta,x)\n",
    "        data = pd.read_excel(wd)\n",
    "        for y in data.columns[1:]:\n",
    "            empresa = y\n",
    "            test_size = 0.3 # longitud del testeo\n",
    "            valor = 1 - test_size\n",
    "            valor1 = valor*len(data[empresa].dropna())\n",
    "            valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "            TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "            epochs = 100\n",
    "            batch_size = 32\n",
    "            \n",
    "            #PREPROCESAMIENTO DE LOS DATOS\n",
    "            data['valor_normalizado'] = data[empresa].dropna().reset_index().drop(columns='index')\n",
    "            scaler = StandardScaler()\n",
    "            scaler = scaler.fit(data[['valor_normalizado']])\n",
    "            data['valor_normalizado'] = scaler.transform(data[['valor_normalizado']])\n",
    "\n",
    "            def create_dataset(X, y, time_steps=1):\n",
    "                Xs, ys = [], []\n",
    "                for i in range(len(X) - time_steps):\n",
    "                    v = X.iloc[i:(i + time_steps)].values\n",
    "                    Xs.append(v)        \n",
    "                    ys.append(y.iloc[i + time_steps])\n",
    "                return np.array(Xs), np.array(ys)\n",
    "\n",
    "            X_data, y_data = create_dataset(data[['valor_normalizado']], data.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "            #predicciones\n",
    "            predictions = model.predict(X_data)\n",
    "            predicciones1 = pd.DataFrame(predictions.flat[0:TIME_STEPS])\n",
    "\n",
    "            predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "            predicciones = pd.concat((predicciones1, predicciones2),axis=0)\n",
    "\n",
    "            predicciones.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #calculo del error de reconstrucción\n",
    "            df_test = pd.DataFrame()\n",
    "            df_test['x_test'] = data['valor_normalizado']\n",
    "            df_test['predic'] = predicciones\n",
    "            df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "            df_test['error2'] = df_test['error']**2\n",
    "\n",
    "            #cálculo del threshold\n",
    "            mu = np.mean(df_test['error2'])  # Media\n",
    "            sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "            alpha = 0.05  # Nivel de significancia del 5%\n",
    "            z_score = stats.norm.ppf(1 - alpha/2) \n",
    "            threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "            anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "            # Crear una figura y un solo eje y\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            # Gráfico de Datos Originales y Predicciones\n",
    "            ax.plot(data['valor_normalizado'], label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "            ax.plot(predicciones, label='Predicciones', color='black')\n",
    "            ax.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=20)\n",
    "            ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "            ax.set_xlabel('Datos de Testeo')\n",
    "            ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "            ax.legend(loc='upper left')\n",
    "            \n",
    "            # Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(df_test.index, df_test.error2, label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "            ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "            ax2.legend(loc='upper right')\n",
    "            y2_min = 0\n",
    "            y2_max = max(df_test.error2)*2\n",
    "            \n",
    "            # Establecer los límites en el eje y del segundo conjunto de datos\n",
    "            ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "            # Ajustar el espaciado entre los gráficos\n",
    "            plt.tight_layout()\n",
    "            # Mostrar el gráfico combinado\n",
    "            plt.savefig(os.path.join(ruta2,f'{x}-{y}.png'))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "empresa = 'claro'\n",
    "\n",
    "\n",
    "test_size = 0.3 # longitud del testeo\n",
    "valor = 1 - test_size\n",
    "valor1 = valor*400\n",
    "valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "epochs = 100 \n",
    "batch_size = 32\n",
    "\n",
    "#PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "data['valor_normalizado'] = data[empresa]\n",
    "train, test = train_test_split(data['valor_normalizado'], test_size=test_size, shuffle = False)\n",
    "train = pd.DataFrame(train)\n",
    "test= pd.DataFrame(test)\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['valor_normalizado']])\n",
    "train['valor_normalizado'] = scaler.transform(train[['valor_normalizado']])\n",
    "test['valor_normalizado'] = scaler.transform(test[['valor_normalizado']])\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_dataset(train[['valor_normalizado']], train.valor_normalizado, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[['valor_normalizado']], test.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "#CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0) #ENTRENAMIENTO\n",
    "mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# desprocesamiento de los datos de testeo\n",
    "a = pd.DataFrame(X_test[0].flat)\n",
    "b = pd.DataFrame(y_test)\n",
    "datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "#predicciones\n",
    "predictions = model.predict(X_test)\n",
    "predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "predicciones = pd.concat((predicciones1, predicciones2))\n",
    "predicciones.reset_index(drop=True, inplace=True)\n",
    "datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#calculo del error de reconstrucción\n",
    "df_test = pd.DataFrame()\n",
    "df_test['x_test'] = datos_testeo\n",
    "df_test['predic'] = predicciones\n",
    "df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "df_test['error2'] = df_test['error']**2\n",
    "\n",
    "#cálculo del threshold\n",
    "mu = np.mean(df_test['error2'])  # Media\n",
    "sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "alpha = 0.05  # Nivel de significancia del 5%\n",
    "z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los espacios de búsqueda para los hiperparámetros\n",
    "    num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "    num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "    num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    # Crear un modelo Autoencoder\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "        tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "    # Compilar el modelo con los hiperparámetros seleccionados\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "    # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "    history = model.fit(X_train, X_train, epochs=100, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0)\n",
    "\n",
    "    # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "    # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "    loss = model.evaluate(X_train, X_train)  # Corregido\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = study.best_params\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "#UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "#CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "    tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train, X_train, epochs=50, batch_size=best_params['batch_size'], validation_data=(X_train, X_train), verbose=0)\n",
    "mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "\n",
    "a = pd.DataFrame(X_test[0].flat)\n",
    "b = pd.DataFrame(y_test)\n",
    "datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "predicciones.reset_index(drop=True, inplace=True)\n",
    "datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['x_test'] = datos_testeo\n",
    "df_test['predic'] = predicciones\n",
    "df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "df_test['error2'] = df_test['error']**2\n",
    "\n",
    "anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "\n",
    "# Crear una figura y un solo eje y\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# Gráfico de Datos Originales y Predicciones\n",
    "ax.plot(datos_testeo, label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "ax.plot(predicciones, label='Predicciones', color='black')\n",
    "ax.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=20)\n",
    "ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "ax.set_xlabel('Datos de Testeo')\n",
    "ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df_test.index, df_test.error2, label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "ax2.legend(loc='upper right')\n",
    "y2_min = 0\n",
    "y2_max = max(df_test.error2)*2\n",
    "\n",
    "# Establecer los límites en el eje y del segundo conjunto de datos\n",
    "ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "\n",
    "# Ajustar el espaciado entre los gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gráfico combinado\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
