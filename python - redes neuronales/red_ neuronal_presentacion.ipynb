{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTACION DE LAS LIBRERIAS A UTILIZAR\n",
    "\n",
    "import pandas as pd #librería para analizar datos\n",
    "import numpy as np #librería para algebra linear\n",
    "import tensorflow as tf #librería para modelos de deep learning\n",
    "import matplotlib.pyplot as plt # libreraía para realizar los graficos\n",
    "import optuna # libreria para optimizar modelo\n",
    "from sklearn.model_selection import train_test_split #parte de la librería que divide los datos\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # libreria para calcular las metricas en la curva ROC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # librerias para calcular la precision, el recall y el f1 score\n",
    "from sklearn.preprocessing import MinMaxScaler #libreria para estandarizacion de datos\n",
    "from sklearn.preprocessing import StandardScaler #libreria para estandarizacion de datos \n",
    "from keras.models import Sequential #importar el inicio de un modelo secuencial\n",
    "from keras.layers import LSTM, Dense # importa capas densas y de LSTM\n",
    "from sklearn.metrics import confusion_matrix #parte de la librería para hacer la matriz de confusion\n",
    "from tensorflow import keras #importa keras que se encarga de la transformacion de los datos\n",
    "from spicy import stats\n",
    "\n",
    "# cargamos datos y fijamos valores del modelo\n",
    "data = pd.read_excel(\"anomalias2_ajustado2.xlsx\")\n",
    "empresa = 'claro'\n",
    "empresa1 = 'claro_l'\n",
    "\n",
    "test_size = 0.3 # longitud del testeo\n",
    "valor = 1 - test_size\n",
    "valor1 = valor*400\n",
    "valor1 = int(valor1) #longitud del la parte de entrenamiento\n",
    "TIME_STEPS = 30 #valor que se puede variar en el tiempo\n",
    "epochs = 100 \n",
    "batch_size = 32\n",
    "etiqueta = data[empresa1][valor1:]\n",
    "#PREPROCESAMIENTO DE LOS DATOS\n",
    "\n",
    "data['valor_normalizado'] = data[empresa]\n",
    "train, test = train_test_split(data['valor_normalizado'], test_size=test_size, shuffle = False)\n",
    "train = pd.DataFrame(train)\n",
    "test= pd.DataFrame(test)\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['valor_normalizado']])\n",
    "train['valor_normalizado'] = scaler.transform(train[['valor_normalizado']])\n",
    "test['valor_normalizado'] = scaler.transform(test[['valor_normalizado']])\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_dataset(train[['valor_normalizado']], train.valor_normalizado, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[['valor_normalizado']], test.valor_normalizado, TIME_STEPS)\n",
    "\n",
    "#CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0) #ENTRENAMIENTO\n",
    "mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# desprocesamiento de los datos de testeo\n",
    "a = pd.DataFrame(X_test[0].flat)\n",
    "b = pd.DataFrame(y_test)\n",
    "datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "#predicciones\n",
    "predictions = model.predict(X_test)\n",
    "predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "predicciones = pd.concat((predicciones1, predicciones2))\n",
    "predicciones.reset_index(drop=True, inplace=True)\n",
    "datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#calculo del error de reconstrucción\n",
    "df_test = pd.DataFrame()\n",
    "df_test['x_test'] = datos_testeo\n",
    "df_test['predic'] = predicciones\n",
    "df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "df_test['error2'] = df_test['error']**2\n",
    "\n",
    "#cálculo del threshold\n",
    "mu = np.mean(df_test['error2'])  # Media\n",
    "sigma = np.std(df_test['error2'])  # Desviación estándar\n",
    "alpha = 0.05  # Nivel de significancia del 5%\n",
    "z_score = stats.norm.ppf(1 - alpha/2)  # Utilizando la función de percentil inverso de la distribución normal\n",
    "threshold = mu - (z_score * (sigma / np.sqrt(len(df_test['error2'])))) #Umbral gaussiano\n",
    "\n",
    "anomalias = df_test[df_test['error2'] > threshold] #creación de la variable anomalias que contiene a aquellos errores mayores al umbral\n",
    "\n",
    "#etiquetas de las anomalias\n",
    "predic = pd.DataFrame()\n",
    "predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "predic.loc[anomalias.index + valor1, 'Valor'] = 1\n",
    "predic = predic[valor1:]\n",
    "confusion = confusion_matrix(etiqueta, predic)\n",
    "\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "precision = precision_score(etiqueta, predic)\n",
    "recall = recall_score(etiqueta, predic)\n",
    "f1 = f1_score(etiqueta, predic)\n",
    "fpr, tpr, thresholds = roc_curve(etiqueta, predic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fpr, tpr, _ = roc_curve(etiqueta, predic)\n",
    "roc_auc = roc_auc_score(etiqueta, predic)\n",
    "\n",
    "print(\"Verdaderos Positivos (TP):\", TP)\n",
    "print(\"Verdaderos Negativos (TN):\", TN)\n",
    "print(\"Falsos Positivos (FP):\", FP)\n",
    "print(\"Falsos Negativos (FN):\", FN)\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Área bajo la curva (AUC): {:.2f}\".format(roc_auc))\n",
    "\n",
    "# Gráfico de Pérdida en Entrenamiento y Validación\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss, label='Pérdida en Entrenamiento', color='blue')\n",
    "plt.plot(val_loss, label='Pérdida en Validación', color='red')\n",
    "plt.legend()\n",
    "plt.title('Gráfico de Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Pérdida y Umbral para Detección de Anomalías\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_test.index, df_test.error2, label='Pérdida', color='blue')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Umbral')\n",
    "plt.title('Pérdida vs. Umbral para Detección de Anomalías')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Datos Originales vs. Predicciones (con Anomalías)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datos_testeo, label='Datos Originales', color='blue')\n",
    "plt.plot(predicciones, label='Predicciones', color='green')\n",
    "plt.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=50)\n",
    "plt.title('Datos Originales vs. Predicciones (con Anomalías)')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#CURVA ROC\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSQUEDA DEL THRESHOLD OPTIMO\n",
    "\n",
    "y = []\n",
    "for x in range(0,10000):\n",
    "    y.append((x/1000))\n",
    "pre = []\n",
    "rec = []\n",
    "f1 = []\n",
    "optimos =[]\n",
    "for i in y:\n",
    "    threshold = i\n",
    "    anomalias = df_test[df_test['error2'] > threshold]\n",
    "    predic = pd.DataFrame()\n",
    "    predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "    predic.loc[anomalias.index+valor1, 'Valor'] = 1\n",
    "    predic = predic[valor1:]\n",
    "    precision = precision_score(etiqueta, predic,zero_division=0)\n",
    "    recall = recall_score(etiqueta, predic,zero_division=0)\n",
    "    f = f1_score(etiqueta, predic,zero_division=0)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pre, label='Precision', color='blue')\n",
    "plt.plot(f1, label='F1 Score', color='green')\n",
    "plt.plot(rec, label='Recall', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Umbral')\n",
    "plt.ylabel('Valor de la Métrica')\n",
    "plt.title('Métricas de Clasificación en función del Umbral')\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte las listas en arrays de numpy para facilitar la manipulación\n",
    "pre = np.array(pre)\n",
    "rec = np.array(rec)\n",
    "f1 = np.array(f1)\n",
    "\n",
    "# Encuentra el índice del umbral que maximiza F1\n",
    "indice_umbral_optimo = np.argmax(f1)\n",
    "\n",
    "# Obtiene el umbral óptimo\n",
    "umbral_optimo = y[indice_umbral_optimo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REEMPLAZAR ###\n",
    "threshold = umbral_optimo\n",
    "anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "predic = pd.DataFrame()\n",
    "predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "predic.loc[anomalias.index + valor1, 'Valor'] = 1\n",
    "predic = predic[valor1:]\n",
    "\n",
    "confusion = confusion_matrix(etiqueta, predic)\n",
    "\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "precision = precision_score(etiqueta, predic)\n",
    "recall = recall_score(etiqueta, predic)\n",
    "f1 = f1_score(etiqueta, predic)\n",
    "fpr, tpr, thresholds = roc_curve(etiqueta, predic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fpr, tpr, _ = roc_curve(etiqueta, predic)\n",
    "roc_auc = roc_auc_score(etiqueta, predic)\n",
    "\n",
    "print(\"Verdaderos Positivos (TP):\", TP)\n",
    "print(\"Verdaderos Negativos (TN):\", TN)\n",
    "print(\"Falsos Positivos (FP):\", FP)\n",
    "print(\"Falsos Negativos (FN):\", FN)\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Área bajo la curva (AUC): {:.2f}\".format(roc_auc))\n",
    "\n",
    "# Gráfico de Pérdida en Entrenamiento y Validación\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss, label='Pérdida en Entrenamiento', color='blue')\n",
    "plt.plot(val_loss, label='Pérdida en Validación', color='red')\n",
    "plt.legend()\n",
    "plt.title('Gráfico de Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Pérdida y Umbral para Detección de Anomalías\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_test.index, df_test.error2, label='Pérdida', color='blue')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Umbral')\n",
    "plt.title('Pérdida vs. Umbral para Detección de Anomalías')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Datos Originales vs. Predicciones (con Anomalías)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datos_testeo, label='Datos Originales', color='blue')\n",
    "plt.plot(predicciones, label='Predicciones', color='green')\n",
    "plt.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=50)\n",
    "plt.title('Datos Originales vs. Predicciones (con Anomalías)')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#CURVA ROC\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Definir los espacios de búsqueda para los hiperparámetros\n",
    "    num_units1 = trial.suggest_int('num_units1', 64, 128)\n",
    "    num_units2 = trial.suggest_int('num_units2', 32, 64)\n",
    "    num_units3 = trial.suggest_int('num_units3', 8, 16)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    # Crear un modelo Autoencoder\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units3, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units2, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_units1, activation='relu'),\n",
    "        tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "    # Compilar el modelo con los hiperparámetros seleccionados\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Corregido\n",
    "\n",
    "    # Entrenar el modelo con los datos de entrenamiento y prueba\n",
    "    history = model.fit(X_train, X_train, epochs=100, batch_size=batch_size, validation_data=(X_train, X_train), verbose=0)\n",
    "\n",
    "    # Calcular la métrica de interés (puede ser la precisión, el AUC, etc.)\n",
    "    # En este ejemplo, usaremos la pérdida en los datos de prueba\n",
    "    loss = model.evaluate(X_train, X_train)  # Corregido\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Crear el estudio de Optuna y ejecutar la minimizacion de la pérdida\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = study.best_params\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZAMOS EL MODELO CON LOS NUEVOS PARÁMETROS, Y BATCHSIZE\n",
    "#CREACION DEL MODELO AUTOENCODER CON CAPAS DENSAS\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1])),\n",
    "    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units3'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units2'], activation='relu'),\n",
    "    tf.keras.layers.Dense(best_params['num_units1'], activation='relu'),\n",
    "    tf.keras.layers.Dense(X_train.shape[1], activation='linear')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train, X_train, epochs=50, batch_size=best_params['batch_size'], validation_data=(X_train, X_train), verbose=0)\n",
    "mse = model.evaluate(X_train, X_train, verbose=0)\n",
    "\n",
    "a = pd.DataFrame(X_test[0].flat)\n",
    "b = pd.DataFrame(y_test)\n",
    "datos_testeo = pd.concat((a,b)).reset_index(drop=True)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicciones1 = pd.DataFrame(predictions[0].flat)\n",
    "predicciones2 = pd.DataFrame(predictions.flat)[(TIME_STEPS-1)::TIME_STEPS]\n",
    "predicciones = pd.concat((predicciones1, predicciones2))\n",
    "\n",
    "predicciones.reset_index(drop=True, inplace=True)\n",
    "datos_testeo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['x_test'] = datos_testeo\n",
    "df_test['predic'] = predicciones\n",
    "df_test['error'] = df_test['x_test'] - df_test['predic']\n",
    "df_test['error2'] = df_test['error']**2\n",
    "\n",
    "predic = pd.DataFrame()\n",
    "predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "predic.loc[anomalias.index + valor1, 'Valor'] = 1\n",
    "predic = predic[valor1:]\n",
    "confusion = confusion_matrix(etiqueta, predic)\n",
    "\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "precision = precision_score(etiqueta, predic)\n",
    "recall = recall_score(etiqueta, predic)\n",
    "f1 = f1_score(etiqueta, predic)\n",
    "fpr, tpr, thresholds = roc_curve(etiqueta, predic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"Verdaderos Positivos (TP):\", TP)\n",
    "print(\"Verdaderos Negativos (TN):\", TN)\n",
    "print(\"Falsos Positivos (FP):\", FP)\n",
    "print(\"Falsos Negativos (FN):\", FN)\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Área bajo la curva (AUC): {:.2f}\".format(roc_auc))\n",
    "\n",
    "# Gráfico de Pérdida en Entrenamiento y Validación\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss, label='Pérdida en Entrenamiento', color='blue')\n",
    "plt.plot(val_loss, label='Pérdida en Validación', color='red')\n",
    "plt.legend()\n",
    "plt.title('Gráfico de Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Pérdida y Umbral para Detección de Anomalías\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_test.index, df_test.error2, label='Pérdida', color='blue')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Umbral')\n",
    "plt.title('Pérdida vs. Umbral para Detección de Anomalías')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Datos Originales vs. Predicciones (con Anomalías)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datos_testeo, label='Datos Originales', color='blue')\n",
    "plt.plot(predicciones, label='Predicciones', color='green')\n",
    "plt.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=50)\n",
    "plt.title('Datos Originales vs. Predicciones (con Anomalías)')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEJORA DEL UMBRAL\n",
    "\n",
    "y = []\n",
    "for x in range(0,10000):\n",
    "    y.append((x/1000))\n",
    "pre = []\n",
    "rec = []\n",
    "f1 = []\n",
    "optimos =[]\n",
    "for i in y:\n",
    "    threshold = i\n",
    "    anomalias = df_test[df_test['error2'] > threshold]\n",
    "    predic = pd.DataFrame()\n",
    "    predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "    predic.loc[anomalias.index+valor1, 'Valor'] = 1\n",
    "    predic = predic[valor1:]\n",
    "    precision = precision_score(etiqueta, predic,zero_division=0)\n",
    "    recall = recall_score(etiqueta, predic,zero_division=0)\n",
    "    f = f1_score(etiqueta, predic,zero_division=0)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f)\n",
    "\n",
    "#Grafico de umbral\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pre, label='Precision', color='blue')\n",
    "plt.plot(f1, label='F1 Score', color='green')\n",
    "plt.plot(rec, label='Recall', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Umbral')\n",
    "plt.ylabel('Valor de la Métrica')\n",
    "plt.title('Métricas de Clasificación en función del Umbral')\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte las listas en arrays de numpy para facilitar la manipulación\n",
    "pre = np.array(pre)\n",
    "rec = np.array(rec)\n",
    "f1 = np.array(f1)\n",
    "# Encuentra el índice del umbral que maximiza f1\n",
    "indice_umbral_optimo = np.argmax(f1)\n",
    "# Obtiene el umbral óptimo\n",
    "umbral_optimo = y[indice_umbral_optimo]\n",
    "# Encuentra el índice del umbral que maximiza recall\n",
    "indice_umbral_optimo = np.argmax(rec)\n",
    "# Obtiene el umbral óptimo\n",
    "umbral_optimo1 = y[indice_umbral_optimo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REEMPLAZAR ###\n",
    "threshold = umbral_optimo\n",
    "anomalias = df_test[df_test['error2'] > threshold] #ANOMALIA DE ACUERDO A LO QUE SEA MAYOR AL THRESHOLD\n",
    "\n",
    "# Gráfico de Pérdida en Entrenamiento y Validación\n",
    "predic = pd.DataFrame()\n",
    "predic = pd.DataFrame({'Valor': [0] * len(data[empresa1])})\n",
    "predic.loc[anomalias.index + valor1, 'Valor'] = 1\n",
    "predic = predic[valor1:]\n",
    "confusion = confusion_matrix(etiqueta, predic)\n",
    "\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "precision = precision_score(etiqueta, predic)\n",
    "recall = recall_score(etiqueta, predic)\n",
    "f1 = f1_score(etiqueta, predic)\n",
    "\n",
    "print(\"Verdaderos Positivos (TP):\", TP)\n",
    "print(\"Verdaderos Negativos (TN):\", TN)\n",
    "print(\"Falsos Positivos (FP):\", FP)\n",
    "print(\"Falsos Negativos (FN):\", FN)\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# Gráfico de perdida y mbral para deteccion de anomalias\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss, label='Pérdida en Entrenamiento', color='blue')\n",
    "plt.plot(val_loss, label='Pérdida en Validación', color='red')\n",
    "plt.legend()\n",
    "plt.title('Gráfico de Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Pérdida y Umbral para Detección de Anomalías\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_test.index, df_test.error2, label='Pérdida', color='blue')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Umbral')\n",
    "plt.title('Pérdida vs. Umbral para Detección de Anomalías')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Datos Originales vs. Predicciones (con Anomalías)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datos_testeo, label='Datos Originales', color='blue')\n",
    "plt.plot(predicciones, label='Predicciones', color='green')\n",
    "plt.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=50)\n",
    "plt.title('Datos Originales vs. Predicciones (con Anomalías)')\n",
    "plt.xlabel('Índice de Datos')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una figura y un solo eje y\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Gráfico de Datos Originales y Predicciones\n",
    "ax.plot(datos_testeo, label='Datos Originales', color='blue', linestyle = '--', linewidth = 0.8)\n",
    "ax.plot(predicciones, label='Predicciones', color='black')\n",
    "ax.scatter(anomalias.index, anomalias['x_test'], color='red', label='Anomalías', marker='o', s=20)\n",
    "ax.set_title('Datos de tráfico vs. Predicciones (con Anomalías)', fontweight = 'bold')\n",
    "ax.set_xlabel('Datos de Testeo')\n",
    "ax.set_ylabel('Valor Normalizado', color='black')  # Eje izquierdo\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Crear un segundo conjunto de etiquetas en el eje y para la pérdida y el umbral\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df_test.index, df_test.error2, label='Pérdida', color='red', linestyle='--', linewidth = 1.5)\n",
    "ax2.set_ylabel('Pérdida y Umbral', color='black')  # Eje derecho\n",
    "ax2.legend(loc='upper right')\n",
    "y2_min = 0\n",
    "y2_max = max(df_test.error2)*2\n",
    "\n",
    "# Establecer los límites en el eje y del segundo conjunto de datos\n",
    "ax2.set_ylim(y2_min, y2_max)  # Reemplaza y2_min y y2_max con los valores deseados\n",
    "\n",
    "# Ajustar el espaciado entre los gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gráfico combinado\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
